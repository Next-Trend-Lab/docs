---
aliases:
- evaluating
- evaluation
---

![[evaluating.png]]

## **Introduction**  
### **The Essence of Our Evaluation Approach**  
Evaluation, for us, is not a static measure of success but a living [[practice]] of listening to the pulse of systems. It is how communities learn to dance with complexity, attuning to the rhythms of social relationality, ecological vitality, cultural wisdom, governance systems and emergent possibility. At Prisma, we see evaluation as the *essence of organisational development*—a way for collectives to sense, adapt, and evolve their actions in harmony with the essence of [[place]] and the [[Living Systems Principles]] that bind us all. 

#### Vision
To redefine evaluation as a dynamic dialogue with life itself—a socio-technological practice that illuminates the flows of value that drive collective evolution.  

#### Mission
To deploy and grow a technology-enabled general framework that adapts to [[place]] & [[practice]] through honoring energy flow in its [multi-capital](Multi-Capitals Framework) forms across [[g. Five Spheres of Evaluation|five spheres]], empowering communities to steward regenerative futures.  

#### Purpose
We exist to:  
- **Make visible** the interconnected value generated by actions undertaken within regenerative and/or action-learning intensives.  
- **Foster agency** in communities to refine their practices, not through top-down metrics, but through shared inquiry: *How do our actions add value to the evolution of life and systems around us?*  
- **Bridge scales** by linking hyperlocal insights (e.g., a village’s water stewardship) to global patterns (e.g., [[bioregion|bioregional]] climate resilience).  

---

### **Principles & Values**  
Our approach is rooted in four guiding stars:  
1. **Humility**: Communities lead the design, ensuring evaluation mirrors their unique [[cultural narrative|narratives]] and rhythms.  
2. **Integrity**: We balance rigor with pragmatism—measuring what matters, not just what’s measurable.  
3. **Emergence**: We seek patterns, not endpoints, embracing the unpredictable alchemy of [[action-learning journeys]].  
4. **Interdependence**: Every metric is a thread in a larger tapestry, woven from relationships between people, place, and practice.  

---

### **Our Story of Evaluation**  
Conventional evaluation asks, *“Did we hit the target?”*  
We ask, *“What is the system telling us? How are we being reshaped by the work?”*  

#### **From Extraction to Enlivening**  
Traditional models reduce value to financial ROI or carbon metrics, severing the connective tissue between ecological, social, and cultural flows. Our [[developmental evaluation]] approach emerged from decades of observing how regenerative systems thrive when communities learn to *read their own vitality*:  
- A forest’s health isn’t just in tree counts, but in the mycelial networks beneath.  
- A community’s resilience isn’t just in GDP, but in the quiet strength of elders sharing stories around fires.  

#### **The Dance of Enactment & Reflection**  
Evaluation here is a dance between [[docs/Glossary/Enactment|doing]] and [[developmental evaluation|sensing]]. During an [[action-learning journey]] in Accra, youth-led recycling initiatives weren’t just measured by tons of plastic diverted. Through participatory storytelling and [[Warm Data Labs|relational mapping]], we surfaced how their work rewired local power dynamics—empowering women traders to co-design waste-to-energy microgrids. *That* is the alchemy of valuing energy flows.  

#### **Tools as Living Systems**  
Our tools—[[Holochain|decentralized governance protocols]], [[Speech-to-Currency]] systems, [[bot arrays]]—are designed to evolve with the communities using them. Like a river shaping its banks, our methodologies adapt to the terrain, guided by questions like:  
- *What does this place need to become more of itself?*  
- *How do we honor both data and intuition in our learning?*  

---

### **Why This Matters**  
In a world addicted to oversimplification, our work is to hold space for complexity. To evaluate regeneratively is to:  
- **See systems whole**, refusing to fracture ecological health from cultural memory.  
- **Trust communities** as the ultimate authors of their stories.  
- **Rewrite the future** by listening deeply to the past.  

This is not just evaluation—it’s a love letter to life’s interconnectedness.  

**Implementation Framework**

In addition to the above perspectives, we have designed our evaluation methodology by bringing together the following elements into a general framework that can be deployed anywhere in a contextual manner:

1. The Logistics
	1. **When (does Data get Collected for Evaluation)**
		1. In response to regular programming
		2. On a regular basis
		3. When emergent events happen
	2. **Which (Touchpoints for Data Collection are Used)**
		4. Verbose input (reflections, feedback)
		5. Point systems (ranking, rating)
		6. Programmatic data (submissions, sensor readings)
	3. **Who (are the Value Generating Entities)**
		1. Individuals (acting as sovereign agents)
		2. Communities (groups of stakeholders)
		3. Systems (as understood through ongoing processes)

The above 3 elements form the **container** for evaluation. These are largely set and do not change from place to place, although the specific nature of each element depends on the case in question.

2. The Variables
	1. **What (is of value *to us*)**
		1. Activities (effort put into doing something)
		2. Knowledge (writing documents, sharing past work)
		3. Space (hosting a session, connecting to a network)
	2. **Why (is it valued *by us*)**
		4. Regenerative Shifts
		5. Organizational Growth
		6. Outputs Generated
	3. **How (is value flowing *through us* seen)**
		1. Actions (as captured through observations, technology)
		2. Reflections (personally shared by stakeholders)
		3. Attributions (peer awarded for different aspects)


The above 3 elements form the **playground** within which the stakeholders experiment with value generation and learning. These are simply examples and comprise the template that we offer to each community.

3. The Delivery
	1. In Stages
		1. Baselining - established before intensive begins
		2. Reading - during the intensive
		3. Inferencing - after the intensive ends
	2. Through Tech
		4. Telegram Bot(s) - soliciting different forms of subjective data
		5. Data Repositories - from GitHub repos to wiki publishing statistics
		6. AI-enabled Analysis & Timelining - deployed on all data collected
	3. Plotted against
		1. Aspects - three to seven forms of primary value generation 
		2. Capitals - five to ten kinds of resources mobilized for intensive
		3. Spheres - realms of systems change within which evolution happens

The above 3 elements form the **instruments** which provide dimensionality to the act of evaluation.

In action, the framework can be articulated as follows:

Starting with the Baselining stage, the facilitators of the action-learning journey work with different stakeholders to develop a foundational picture of the five systemic spheres as they currently exist for the community, and a list of activities that will be used as the primary means of understanding how value is being generated. These activities, termed Aspects, will be drawn upon by the stakeholders to resource from and contribute to the different forms of capital that are of relevance to the action learning journey's goals. Over the course of different activities that are undertaken as part of the programming, as well as emergent events that unfold, data will be collected in different forms such as through telegram bots and facilitator observations. This data will be analyzed with the support of an artificial intelligence engine trained to track multi-capital flows and where possible, live insights will be utilized for the purpose of enhancing facilitation and stakeholder participation.

---
