
> Summary:
> what is actually of value
> how does value flow
> what can we tell about the system based on the kinds and flows of value
  
What is evaluation?

For 
You are probably thinking about either:
- impact measurement. 
- performance reviews and KPIs.
- gauging the efficiency of processes.

What is the essence of evaluation for you? 

**E-Valuation**

To us at Prisma, evaluation is a practice. Just like acting is, capacity-building is, writing is, and all other practices that make up any collective endeavour. 

What would the *verb in motion* be for evaluation, if you didn't think about it.

It would be *valuing*.

Evaluation, therefore, can be considered the act of 'valuing'. 

And the 'e'? It stands, as it does in science - for *energy*.

E-valuation is the practice of valuing energy flows. This is ultimately what any stakeholder is doing when they evaluate any effort that has been undertaken. They are considering the energy that has been put in, comparing the outcomes generated against what was desired, and conducting an analysis of the value generated.

**Purpose**

At prisma, our mission is to enabling collectives to undertake intentional action-learning journeys, imbue them with regenerative potential, and make visible the value generated therein for the evolution of multiple levels of stakeholders and orders of systems.

The purpose of evaluation then, is to assist this endeavor in three ways:

1. To enable the groups undertaking these journeys to be clear about **what they are valuing**
2. To provide tools that **make visible the value being generated**, and inform analyses and insights that enhance future group endeavors as well as the broader ecosystem of systems change experiments.
3. To enhance the ability of the group to **grow as a system** by weaving in insights from their own behavior into organizational and personal development

**Perspective**

As should be evident from the broader narrative of prisma, we are grounded in an **ethos of co-creation**. That is to say, we are vehemently opposed to the notion of prisma being seen as a parachuting service provider that is flown in by an organization to meet a need, deploys its machinery, derives the required results and leaves with little to no long-term programming in place.

Rather, we see our work as that of facilitators who work with the People, Place and Practice to concoct the ideal recipes for long term, systemic catalysis. 

As such, we hold the following perspectives towards evaluation:

1. **Co-creative**: What shall be evaluated (variables), how it will be measured (metrics), what level of growth is expected (standards) are all decided *in consensus with the community* - with processes tailored to match local decision-making practices and actively pulled towards greater self-organization.

2. **Multi-perspectival:** While we are stubborn about co-creation, we are also stubborn about ensuring a multi-capital lens to valuing flows of energy. Our approach is premised on *seeing, valuing and developing all forms of capital* - not just financial and economic.

3. **Developmental**: We see evaluation as an ongoing act, not just something that begins when the action is complete. Our tooling is designed to start valuing flows the moment they begin, even before the journey starts, and we are building our capacity to provide stakeholders with live signals and insights into value generation.

4. **Regenerative**: The direction of our evaluation methodology is towards seeing potential, rather than measuring effectiveness. We are looking for how (& why) different components of the system are growing, and not just temporary manifestations designed to meet short-term needs. *We are placing the capacity, potential and will of the people, the place and life itself at the center.*

**Implementation Framework**

In addition to the above perspectives, we have designed our evaluation methodology by bringing together the following elements into a general framework that can be deployed anywhere in a contextual manner:

1. The Logistics
	1. **When (does Data get Collected for Evaluation)**
		1. In response to regular programming
		2. On a regular basis
		3. When emergent events happen
	2. **Which (Touchpoints for Data Collection are Used)**
		1. Verbose input (reflections, feedback)
		2. Point systems (ranking, rating)
		3. Programmatic data (submissions, sensor readings)
	3. **Who (are the Value Generating Entities)**
		1. Individuals (acting as sovereign agents)
		2. Communities (groups of stakeholders)
		3. Systems (as understood through ongoing processes)

The above 3 elements form the **container** for evaluation. These are largely set and do not change from place to place, although the specific nature of each element depends on the case in question.

2. The Variables
	1. **What (is of value *to us*)**
		1. Activities (effort put into doing something)
		2. Knowledge (writing documents, sharing past work)
		3. Space (hosting a session, connecting to a network)
	2. **Why (is it valued *by us*)**
		1. Regenerative Shifts
		2. Organizational Growth
		3. Outputs Generated
	3. **How (is value flowing *through us* seen)**
		1. Actions (as captured through observations, technology)
		2. Reflections (personally shared by stakeholders)
		3. Attributions (peer awarded for different aspects)


The above 3 elements form the **playground** within which the stakeholders experiment with value generation and learning. These are simply examples and comprise the template that we offer to each community.

3. The Delivery
	1. In Stages
		1. Baselining - established before intensive begins
		2. Reading - during the intensive
		3. Inferencing - after the intensive ends
	2. Through Tech
		1. Telegram Bot(s) - soliciting different forms of subjective data
		2. Data Repositories - from GitHub repos to wiki publishing statistics
		3. AI-enabled Analysis & Timelining - deployed on all data collected
	3. Plotted against
		1. Aspects - three to seven forms of primary value generation 
		2. Capitals - five to ten kinds of resources mobilized for intensive
		3. Spheres - realms of systems change within which evolution happens

The above 3 elements form the **instruments** which provide dimensionality to the act of evaluation.

In action, the framework can be articulated as follows:

Starting with the Baselining stage, the facilitators of the action-learning journey work with different stakeholders to develop a foundational picture of the five systemic spheres as they currently exist for the community, and a list of activities that will be used as the primary means of understanding how value is being generated. These activities, termed Aspects, will be drawn upon by the stakeholders to resource from and contribute to the different forms of capital that are of relevance to the action learning journey's goals. Over the course of different activities that are undertaken as part of the programming, as well as emergent events that unfold, data will be collected in different forms such as through telegram bots and facilitator observations. This data will be analyzed with the support of an artificial intelligence engine trained to track multi-capital flows and where possible, live insights will be utilized for the purpose of enhancing facilitation and stakeholder participation.