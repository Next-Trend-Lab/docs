---
aliases:
- evaluating
- evaluation
---

![[evaluating.png]]

## **Introduction**  
### **The Essence of Our Evaluation Approach**  
Evaluation, for us, is not a static measure of success but a living [[practice]] of listening to the pulse of systems. It is how communities learn to dance with complexity, attuning to the rhythms of social relationality, ecological vitality, cultural wisdom, governance systems and emergent possibility. At Prisma, we see evaluation as the *essence of organisational development*—a way for collectives to sense, adapt, and evolve their actions in harmony with the essence of [[place]] and the [[Living Systems Principles]] that bind us all. 

#### Vision
To redefine evaluation as a dynamic dialogue with life itself—a socio-technological practice that illuminates the flows of value that drive collective evolution.  

#### Mission
To deploy and grow a technology-enabled general framework that adapts to [[place]] & [[practice]] through honoring energy flow in its [multi-capital](Multi-Capitals Framework) forms and provides insight into evolution across a system's [[g. Five Spheres of Evaluation|five spheres]], empowering communities to steward regenerative futures.  

#### Responsibilities
We hold ourselves accountable to answering the following questions:  
1. what alternate forms of data on diverse value contributions be authentically reflected?
2. how can a regenerative evaluation process be conducted?
3. can multi-capital forms of value reading enable holistic evaluation?
4. which tools and practices lower the threshold for active, playful participation in the evaluation processes? 

---

### **Values**  
Our approach is built on the foundations of the following four pillars:  
1. **Playfulness**: Everything is an experiment - true emergence is a factor of trying out different approaches and tools.  
2. **Curiosity**: We seek patterns, exploring how it all comes together - metrics, people, places, practices and the relationships therein.
3. **Reciprocity**: 
4. **Integrity**: We measure what matters - to communities who design their own evaluation methodologies to act as mirrors for their unique narratives and rhythms.

---

**Implementation Framework**

In addition to the above perspectives, we have designed our evaluation methodology by bringing together the following elements into a general framework that can be deployed anywhere in a contextual manner:

1. The Logistics
	1. **When (does Data get Collected for Evaluation)**
		1. In response to regular programming
		2. On a regular basis
		3. When emergent events happen
	2. **Which (Touchpoints for Data Collection are Used)**
		4. Verbose input (reflections, feedback)
		5. Point systems (ranking, rating)
		6. Programmatic data (submissions, sensor readings)
	3. **Who (are the Value Generating Entities)**
		1. Individuals (acting as sovereign agents)
		2. Communities (groups of stakeholders)
		3. Systems (as understood through ongoing processes)

The above 3 elements form the **container** for evaluation. These are largely set and do not change from place to place, although the specific nature of each element depends on the case in question.

2. The Variables
	1. **What (is of value *to us*)**
		1. Activities (effort put into doing something)
		2. Knowledge (writing documents, sharing past work)
		3. Space (hosting a session, connecting to a network)
	2. **Why (is it valued *by us*)**
		4. Regenerative Shifts
		5. Organizational Growth
		6. Outputs Generated
	3. **How (is value flowing *through us* seen)**
		1. Actions (as captured through observations, technology)
		2. Reflections (personally shared by stakeholders)
		3. Attributions (peer awarded for different aspects)


The above 3 elements form the **playground** within which the stakeholders experiment with value generation and learning. These are simply examples and comprise the template that we offer to each community.

3. The Delivery
	1. In Stages
		1. Baselining - established before intensive begins
		2. Reading - during the intensive
		3. Inferencing - after the intensive ends
	2. Through Tech
		4. Telegram Bot(s) - soliciting different forms of subjective data
		5. Data Repositories - from GitHub repos to wiki publishing statistics
		6. AI-enabled Analysis & Timelining - deployed on all data collected
	3. Plotted against
		1. Aspects - three to seven forms of primary value generation 
		2. Capitals - five to ten kinds of resources mobilized for intensive
		3. Spheres - realms of systems change within which evolution happens

The above 3 elements form the **instruments** which provide dimensionality to the act of evaluation.

In action, the framework can be articulated as follows:

Starting with the Baselining stage, the facilitators of the action-learning journey work with different stakeholders to develop a foundational picture of the five systemic spheres as they currently exist for the community, and a list of activities that will be used as the primary means of understanding how value is being generated. These activities, termed Aspects, will be drawn upon by the stakeholders to resource from and contribute to the different forms of capital that are of relevance to the action learning journey's goals. Over the course of different activities that are undertaken as part of the programming, as well as emergent events that unfold, data will be collected in different forms such as through telegram bots and facilitator observations. This data will be analyzed with the support of an artificial intelligence engine trained to track multi-capital flows and where possible, live insights will be utilized for the purpose of enhancing facilitation and stakeholder participation.

---
